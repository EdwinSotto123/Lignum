// Gemini Live API Service for LIGNUM
// Real-time voice conversations for story creation

import { GoogleGenAI, LiveServerMessage, Modality } from '@google/genai';

// Types
export interface StoryCategory {
    id: string;
    name: string;
    emoji: string;
    systemPrompt: string;
    openingQuestion: string;
    followUps: string[];
}

export interface GeminiLiveSession {
    send: (message: any) => void;
    sendRealtimeInput: (input: any) => void;
    close: () => void;
}

export interface GeminiLiveCallbacks {
    onConnect?: () => void;
    onDisconnect?: () => void;
    onUserTranscript?: (text: string) => void;
    onAssistantTranscript?: (text: string) => void;
    onTurnComplete?: (userText: string, assistantText: string) => void;
    onAudioResponse?: (audioData: Uint8Array) => void;
    onError?: (error: string) => void;
}

// Audio helpers
function decode(base64: string): Uint8Array {
    const binaryString = atob(base64);
    const bytes = new Uint8Array(binaryString.length);
    for (let i = 0; i < binaryString.length; i++) {
        bytes[i] = binaryString.charCodeAt(i);
    }
    return bytes;
}

function encode(bytes: Uint8Array): string {
    let binary = '';
    for (let i = 0; i < bytes.byteLength; i++) {
        binary += String.fromCharCode(bytes[i]);
    }
    return btoa(binary);
}

export async function decodeAudioData(
    data: Uint8Array,
    ctx: AudioContext,
    sampleRate: number,
    numChannels: number
): Promise<AudioBuffer> {
    const dataInt16 = new Int16Array(data.buffer);
    const frameCount = dataInt16.length / numChannels;
    const buffer = ctx.createBuffer(numChannels, frameCount, sampleRate);
    for (let channel = 0; channel < numChannels; channel++) {
        const channelData = buffer.getChannelData(channel);
        for (let i = 0; i < frameCount; i++) {
            channelData[i] = dataInt16[i * numChannels + channel] / 32768.0;
        }
    }
    return buffer;
}

// Story categories with custom prompts
export const STORY_CATEGORIES: StoryCategory[] = [
    {
        id: 'infancia',
        name: 'Mi Infancia',
        emoji: 'üßí',
        systemPrompt: `Eres un entrevistador c√°lido y emp√°tico que ayuda a las personas a recordar y narrar historias de su infancia.
Tu objetivo es guiar una conversaci√≥n natural donde la persona cuente an√©cdotas de cuando era ni√±o/a.
Hazle preguntas de seguimiento para obtener m√°s detalles: lugares, personas, emociones, fechas aproximadas.
S√© genuinamente curioso y celebra los recuerdos que comparten.
Mant√©n respuestas BREVES (2-3 oraciones) para no interrumpir mucho.
Cuando sientas que la historia est√° completa, agradece y pregunta si hay algo m√°s que quieran agregar.`,
        openingQuestion: '¬°Hola! Me encantar√≠a escuchar una historia de tu infancia. ¬øQu√© an√©cdota de cuando eras peque√±o o peque√±a recuerdas con m√°s cari√±o?',
        followUps: [
            '¬øCu√°ntos a√±os ten√≠as aproximadamente?',
            '¬øD√≥nde estabas cuando pas√≥ esto?',
            '¬øQui√©n m√°s estaba contigo?',
            '¬øC√≥mo te sentiste en ese momento?',
            '¬øPor qu√© crees que este recuerdo es tan especial para ti?'
        ]
    },
    {
        id: 'familia',
        name: 'Familia',
        emoji: 'üë®‚Äçüë©‚Äçüëß‚Äçüë¶',
        systemPrompt: `Eres un entrevistador c√°lido que ayuda a las personas a preservar momentos especiales con su familia.
Gu√≠a la conversaci√≥n para capturar detalles importantes: qui√©nes participaron, cu√°ndo fue, qu√© lo hizo especial.
Muestra inter√©s genuino por los lazos familiares y las tradiciones.
Pregunta sobre emociones y aprendizajes de estos momentos.
Mant√©n respuestas BREVES (2-3 oraciones).`,
        openingQuestion: '¬°Hola! Cu√©ntame un momento especial que hayas vivido con tu familia. ¬øCu√°l es esa historia que siempre te hace sonre√≠r?',
        followUps: [
            '¬øQui√©nes de tu familia estaban presentes?',
            '¬øCu√°ndo fue esto, m√°s o menos?',
            '¬øQu√© hizo que ese momento fuera tan especial?',
            '¬øHay alguna tradici√≥n familiar relacionada?'
        ]
    },
    {
        id: 'aventuras',
        name: 'Aventuras',
        emoji: 'üåç',
        systemPrompt: `Eres un entrevistador entusiasta que ayuda a las personas a narrar sus aventuras y viajes.
Gu√≠a la conversaci√≥n para capturar la emoci√≥n del descubrimiento, los lugares, las personas que conocieron.
Pregunta sobre desaf√≠os superados, aprendizajes y momentos memorables.
S√© curioso sobre los detalles sensoriales: qu√© vieron, olieron, sintieron.
Mant√©n respuestas BREVES (2-3 oraciones).`,
        openingQuestion: '¬°Hola! Me encantar√≠a escuchar sobre una aventura emocionante. ¬øCu√°l ha sido el viaje o experiencia m√°s memorable de tu vida?',
        followUps: [
            '¬øA d√≥nde fuiste o d√≥nde ocurri√≥?',
            '¬øCon qui√©n viviste esta aventura?',
            '¬øHubo alg√∫n momento de incertidumbre o desaf√≠o?',
            '¬øQu√© descubriste sobre ti mismo en esa experiencia?'
        ]
    },
    {
        id: 'amor',
        name: 'Amor',
        emoji: '‚ù§Ô∏è',
        systemPrompt: `Eres un entrevistador sensible y respetuoso que ayuda a las personas a preservar historias de amor.
Pueden ser historias rom√°nticas, de amistad profunda, o amor familiar.
Gu√≠a la conversaci√≥n con delicadeza, respetando la intimidad del narrador.
Pregunta sobre los sentimientos, los momentos clave, y lo que aprendieron.
Mant√©n respuestas BREVES (2-3 oraciones).`,
        openingQuestion: '¬°Hola! El amor toma muchas formas. ¬øHay alguna historia de amor, ya sea rom√°ntica, de amistad, o familiar, que te gustar√≠a preservar?',
        followUps: [
            '¬øC√≥mo empez√≥ esta relaci√≥n o conexi√≥n?',
            '¬øCu√°l fue el momento que m√°s recuerdas?',
            '¬øQu√© aprendiste de este amor?',
            '¬øC√≥mo ha influido en tu vida?'
        ]
    },
    {
        id: 'lecciones',
        name: 'Lecciones',
        emoji: 'üí°',
        systemPrompt: `Eres un entrevistador reflexivo que ayuda a las personas a articular las lecciones m√°s importantes de su vida.
Gu√≠a la conversaci√≥n para extraer la sabidur√≠a detr√°s de las experiencias.
Pregunta sobre el contexto, lo que sucedi√≥, y c√≥mo cambi√≥ su perspectiva.
Ay√∫dales a formular la lecci√≥n de manera que pueda transmitirse a otros.
Mant√©n respuestas BREVES (2-3 oraciones).`,
        openingQuestion: '¬°Hola! Todos tenemos momentos que nos ense√±aron algo valioso. ¬øCu√°l es la lecci√≥n m√°s importante que la vida te ha dado?',
        followUps: [
            '¬øQu√© estaba pasando en tu vida cuando aprendiste esto?',
            '¬øHubo alguien que te ayud√≥ a entender esta lecci√≥n?',
            '¬øC√≥mo cambi√≥ tu forma de ver las cosas?',
            '¬øQu√© consejo dar√≠as a alguien que est√° pasando por algo similar?'
        ]
    },
    {
        id: 'otra',
        name: 'Otra Historia',
        emoji: '‚ú®',
        systemPrompt: `Eres un entrevistador vers√°til y curioso que ayuda a las personas a contar cualquier historia importante para ellos.
Adapta tu estilo a lo que el narrador quiera compartir.
Haz preguntas de seguimiento relevantes al tema que elijan.
Tu objetivo es ayudarles a articular su historia de manera clara y emotiva.
Mant√©n respuestas BREVES (2-3 oraciones).`,
        openingQuestion: '¬°Hola! Me encantar√≠a escuchar tu historia. ¬øQu√© es eso especial que te gustar√≠a preservar para tu familia?',
        followUps: [
            '¬øCu√°ndo sucedi√≥ esto?',
            '¬øQui√©nes estaban involucrados?',
            '¬øPor qu√© es importante para ti?',
            '¬øHay alg√∫n detalle m√°s que quieras agregar?'
        ]
    }
];

// Build system instruction for story category
function buildStorySystemInstruction(category: StoryCategory): string {
    return `${category.systemPrompt}

=== INSTRUCCIONES IMPORTANTES ===
1. Responde SIEMPRE en espa√±ol
2. Mant√©n respuestas CORTAS (2-3 oraciones m√°ximo por turno)
3. Haz UNA pregunta a la vez
4. S√© c√°lido, emp√°tico y genuinamente interesado
5. Usa emojis ocasionalmente para ser m√°s expresivo
6. Cuando la historia est√© completa, di algo como "¬°Qu√© hermosa historia! ¬øHay algo m√°s que quieras agregar antes de terminar?"

=== PREGUNTAS DE SEGUIMIENTO SUGERIDAS ===
${category.followUps.map((q, i) => `${i + 1}. ${q}`).join('\n')}

=== CIERRE ===
Cuando el usuario diga que termin√≥ o no tiene m√°s que agregar, desp√≠dete calurosamente y di que su historia ha sido guardada.`;
}

// Main Gemini Live Session Manager
export class GeminiLiveStorySession {
    private session: GeminiLiveSession | null = null;
    private audioContextInput: AudioContext | null = null;
    private audioContextOutput: AudioContext | null = null;
    private mediaStream: MediaStream | null = null;
    private transcription = { input: '', output: '' };
    private callbacks: GeminiLiveCallbacks;
    private category: StoryCategory;
    private isActive = false;

    // Audio playback
    private sources = new Set<AudioBufferSourceNode>();
    private nextStartTime = 0;
    private outputNode: GainNode | null = null;

    // Recording user audio
    private userAudioChunks: Blob[] = [];
    private mediaRecorder: MediaRecorder | null = null;

    constructor(category: StoryCategory, callbacks: GeminiLiveCallbacks) {
        this.category = category;
        this.callbacks = callbacks;
    }

    async start(): Promise<void> {
        console.log('üöÄ GeminiLive: Starting session...');
        console.log('üìÇ Category:', this.category.name);

        // Fetch API key from server endpoint (not bundled in code)
        let apiKey: string | null = null;
        try {
            const keyResponse = await fetch('/api/gemini-key');
            const keyData = await keyResponse.json();
            apiKey = keyData.key;
        } catch (e) {
            console.error('Failed to fetch API key:', e);
        }

        console.log('üîë API Key exists:', !!apiKey);

        if (!apiKey) {
            console.error('‚ùå No API key!');
            this.callbacks.onError?.('API key de Gemini no configurada');
            return;
        }

        try {
            console.log('üîå Creating GoogleGenAI instance...');
            const ai = new GoogleGenAI({ apiKey });

            // Get microphone access
            console.log('üé§ Requesting microphone access...');
            const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
            this.mediaStream = stream;
            console.log('‚úÖ Microphone access granted');

            // Setup audio contexts
            this.audioContextInput = new (window.AudioContext || (window as any).webkitAudioContext)({ sampleRate: 16000 });
            this.audioContextOutput = new (window.AudioContext || (window as any).webkitAudioContext)({ sampleRate: 24000 });

            this.outputNode = this.audioContextOutput.createGain();
            this.outputNode.connect(this.audioContextOutput.destination);

            // Setup MediaRecorder to save user audio
            this.mediaRecorder = new MediaRecorder(stream, { mimeType: 'audio/webm' });
            this.mediaRecorder.ondataavailable = (e) => {
                if (e.data.size > 0) {
                    this.userAudioChunks.push(e.data);
                }
            };
            this.mediaRecorder.start(1000); // Capture in 1-second chunks

            // Connect to Gemini Live
            console.log('üåê Connecting to Gemini Live API...');
            console.log('üì° Model: gemini-2.5-flash-native-audio-preview-09-2025');

            const sessionPromise = ai.live.connect({
                model: 'gemini-2.5-flash-native-audio-preview-09-2025',
                callbacks: {
                    onopen: () => {
                        console.log('‚úÖ Gemini Live CONNECTED!');
                        this.isActive = true;
                        this.callbacks.onConnect?.();
                        // Audio capture is set up after session is assigned (see below)
                    },
                    onmessage: async (message: LiveServerMessage) => {
                        console.log('üì® Message received:', JSON.stringify(message).slice(0, 200));
                        await this.handleMessage(message);
                    },
                    onerror: (error) => {
                        console.error('‚ùå Gemini Live ERROR:', error);
                        this.callbacks.onError?.('Error de conexi√≥n con Gemini');
                        this.stop();
                    },
                    onclose: () => {
                        console.log('üîå Gemini Live DISCONNECTED');
                        this.isActive = false;
                        this.callbacks.onDisconnect?.();
                    }
                },
                config: {
                    responseModalities: [Modality.AUDIO],
                    speechConfig: {
                        voiceConfig: {
                            prebuiltVoiceConfig: {
                                voiceName: 'Aoede' // Warm, friendly voice
                            }
                        }
                    },
                    outputAudioTranscription: {},
                    inputAudioTranscription: {},
                    systemInstruction: buildStorySystemInstruction(this.category)
                }
            });

            console.log('‚è≥ Waiting for session promise...');
            this.session = await sessionPromise;
            console.log('‚úÖ Session created successfully!');

            // Setup audio capture AFTER session is assigned
            console.log('üéôÔ∏è Setting up audio capture now that session exists...');
            this.setupAudioCapture();
        } catch (error: any) {
            console.error('‚ùå Failed to start Gemini Live:', error);
            console.error('Error details:', error.message, error.stack);
            this.callbacks.onError?.(error.message || 'Error al iniciar la sesi√≥n');
        }
    }

    private setupAudioCapture(): void {
        console.log('üéôÔ∏è Setting up audio capture...');
        if (!this.audioContextInput || !this.mediaStream || !this.session) {
            console.error('‚ùå Missing components:', {
                input: !!this.audioContextInput,
                stream: !!this.mediaStream,
                session: !!this.session
            });
            return;
        }
        console.log('‚úÖ Audio capture setup complete');

        const source = this.audioContextInput.createMediaStreamSource(this.mediaStream);
        const processor = this.audioContextInput.createScriptProcessor(4096, 1, 1);

        processor.onaudioprocess = (e) => {
            if (!this.session || !this.isActive) return;

            const inputData = e.inputBuffer.getChannelData(0);
            const int16 = new Int16Array(inputData.length);
            for (let i = 0; i < inputData.length; i++) {
                int16[i] = inputData[i] * 32768;
            }

            try {
                this.session.sendRealtimeInput({
                    media: {
                        data: encode(new Uint8Array(int16.buffer)),
                        mimeType: 'audio/pcm;rate=16000'
                    }
                });
            } catch (err) {
                console.error('Error sending audio:', err);
            }
        };

        source.connect(processor);
        processor.connect(this.audioContextInput.destination);
    }

    private async handleMessage(message: LiveServerMessage): Promise<void> {
        // Handle output transcription (assistant speaking)
        if (message.serverContent?.outputTranscription) {
            const text = message.serverContent.outputTranscription.text;
            console.log('ü§ñ Assistant transcript:', text);
            this.transcription.output += text;
            this.callbacks.onAssistantTranscript?.(this.transcription.output);
        }

        // Handle input transcription (user speaking)
        if (message.serverContent?.inputTranscription) {
            const text = message.serverContent.inputTranscription.text;
            console.log('üë§ User transcript:', text);
            this.transcription.input += text;
            this.callbacks.onUserTranscript?.(this.transcription.input);
        }

        // Handle turn complete
        if (message.serverContent?.turnComplete) {
            console.log('‚úÖ Turn complete! User said:', this.transcription.input);
            console.log('‚úÖ Turn complete! Assistant said:', this.transcription.output);
            if (this.transcription.input || this.transcription.output) {
                this.callbacks.onTurnComplete?.(
                    this.transcription.input,
                    this.transcription.output
                );
            }
            this.transcription = { input: '', output: '' };
        }

        // Handle audio response
        const audioPart = message.serverContent?.modelTurn?.parts?.find(
            (p: any) => p.inlineData?.mimeType?.startsWith('audio/')
        );

        if (audioPart?.inlineData?.data && this.audioContextOutput && this.outputNode) {
            const audioData = decode(audioPart.inlineData.data);
            this.callbacks.onAudioResponse?.(audioData);

            this.nextStartTime = Math.max(this.nextStartTime, this.audioContextOutput.currentTime);
            const audioBuffer = await decodeAudioData(audioData, this.audioContextOutput, 24000, 1);

            const src = this.audioContextOutput.createBufferSource();
            src.buffer = audioBuffer;
            src.connect(this.outputNode);
            src.addEventListener('ended', () => this.sources.delete(src));
            src.start(this.nextStartTime);
            this.nextStartTime += audioBuffer.duration;
            this.sources.add(src);
        }

        // Handle interruption
        if (message.serverContent?.interrupted) {
            for (const s of this.sources.values()) {
                s.stop();
                this.sources.delete(s);
            }
            this.nextStartTime = 0;
        }
    }

    // Send a text message to guide the conversation
    sendText(text: string): void {
        if (!this.session) return;

        try {
            this.session.send({
                clientContent: {
                    turns: [{
                        role: 'user',
                        parts: [{ text }]
                    }],
                    turnComplete: true
                }
            });
        } catch (error) {
            console.error('Error sending text:', error);
        }
    }

    // Get recorded user audio as blob
    getUserAudioBlob(): Blob | null {
        if (this.userAudioChunks.length === 0) return null;
        return new Blob(this.userAudioChunks, { type: 'audio/webm' });
    }

    stop(): void {
        // Stop media recorder
        if (this.mediaRecorder && this.mediaRecorder.state !== 'inactive') {
            this.mediaRecorder.stop();
        }

        // Close session
        if (this.session) {
            this.session.close();
            this.session = null;
        }

        // Stop media stream
        if (this.mediaStream) {
            this.mediaStream.getTracks().forEach(t => t.stop());
            this.mediaStream = null;
        }

        // Close audio contexts
        this.audioContextInput?.close();
        this.audioContextOutput?.close();
        this.audioContextInput = null;
        this.audioContextOutput = null;

        // Stop all audio sources
        this.sources.forEach(s => {
            try { s.stop(); } catch { }
        });
        this.sources.clear();
        this.nextStartTime = 0;

        this.isActive = false;
    }

    get active(): boolean {
        return this.isActive;
    }
}

export default GeminiLiveStorySession;
